{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"demo\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "s = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"file:///root/ipynb/departments.csv\"\n",
    "rdd = s.textFile(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEPARTMENT_ID,DEPARTMENT_NAME,MANAGER_ID,LOCATION_ID',\n",
       " '10,Administration,200,1700',\n",
       " '20,Marketing,201,1800',\n",
       " '30,Purchasing,114,1700',\n",
       " '40,Human Resources,203,2400',\n",
       " '50,Shipping,121,1500',\n",
       " '60,IT,103,1400',\n",
       " '70,Public Relations,204,2700',\n",
       " '80,Sales,145,2500',\n",
       " '90,Executive,100,1700',\n",
       " '100,Finance,108,1700',\n",
       " '110,Accounting,205,1700',\n",
       " '120,Treasury, - ,1700',\n",
       " '130,Corporate Tax, - ,1700',\n",
       " '140,Control And Credit, - ,1700',\n",
       " '150,Shareholder Services, - ,1700',\n",
       " '160,Benefits, - ,1700',\n",
       " '170,Manufacturing, - ,1700',\n",
       " '180,Construction, - ,1700',\n",
       " '190,Contracting, - ,1700',\n",
       " '200,Operations, - ,1700',\n",
       " '210,IT Support, - ,1700',\n",
       " '220,NOC, - ,1700',\n",
       " '230,IT Helpdesk, - ,1700',\n",
       " '240,Government Sales, - ,1700',\n",
       " '250,Retail Sales, - ,1700',\n",
       " '260,Recruiting, - ,1700',\n",
       " '270,Payroll, - ,1700']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10,Administration,200,1700',\n",
       " '20,Marketing,201,1800',\n",
       " '30,Purchasing,114,1700',\n",
       " '40,Human Resources,203,2400',\n",
       " '50,Shipping,121,1500',\n",
       " '60,IT,103,1400',\n",
       " '70,Public Relations,204,2700',\n",
       " '80,Sales,145,2500',\n",
       " '90,Executive,100,1700',\n",
       " '100,Finance,108,1700',\n",
       " '110,Accounting,205,1700',\n",
       " '120,Treasury, - ,1700',\n",
       " '130,Corporate Tax, - ,1700',\n",
       " '140,Control And Credit, - ,1700',\n",
       " '150,Shareholder Services, - ,1700',\n",
       " '160,Benefits, - ,1700',\n",
       " '170,Manufacturing, - ,1700',\n",
       " '180,Construction, - ,1700',\n",
       " '190,Contracting, - ,1700',\n",
       " '200,Operations, - ,1700',\n",
       " '210,IT Support, - ,1700',\n",
       " '220,NOC, - ,1700',\n",
       " '230,IT Helpdesk, - ,1700',\n",
       " '240,Government Sales, - ,1700',\n",
       " '250,Retail Sales, - ,1700',\n",
       " '260,Recruiting, - ,1700',\n",
       " '270,Payroll, - ,1700']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Header rdd\n",
    "header_rdd = rdd.first()\n",
    "data_rdd = rdd.filter(lambda row: row != header_rdd)\n",
    "data_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['10', 'Administration', '200', '1700'],\n",
       " ['20', 'Marketing', '201', '1800'],\n",
       " ['30', 'Purchasing', '114', '1700'],\n",
       " ['40', 'Human Resources', '203', '2400'],\n",
       " ['50', 'Shipping', '121', '1500'],\n",
       " ['60', 'IT', '103', '1400'],\n",
       " ['70', 'Public Relations', '204', '2700'],\n",
       " ['80', 'Sales', '145', '2500'],\n",
       " ['90', 'Executive', '100', '1700'],\n",
       " ['100', 'Finance', '108', '1700'],\n",
       " ['110', 'Accounting', '205', '1700'],\n",
       " ['120', 'Treasury', ' - ', '1700'],\n",
       " ['130', 'Corporate Tax', ' - ', '1700'],\n",
       " ['140', 'Control And Credit', ' - ', '1700'],\n",
       " ['150', 'Shareholder Services', ' - ', '1700'],\n",
       " ['160', 'Benefits', ' - ', '1700'],\n",
       " ['170', 'Manufacturing', ' - ', '1700'],\n",
       " ['180', 'Construction', ' - ', '1700'],\n",
       " ['190', 'Contracting', ' - ', '1700'],\n",
       " ['200', 'Operations', ' - ', '1700'],\n",
       " ['210', 'IT Support', ' - ', '1700'],\n",
       " ['220', 'NOC', ' - ', '1700'],\n",
       " ['230', 'IT Helpdesk', ' - ', '1700'],\n",
       " ['240', 'Government Sales', ' - ', '1700'],\n",
       " ['250', 'Retail Sales', ' - ', '1700'],\n",
       " ['260', 'Recruiting', ' - ', '1700'],\n",
       " ['270', 'Payroll', ' - ', '1700']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map\n",
    "values_rdd = data_rdd.map(lambda row: row.split(','))\n",
    "values_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['10', 'Administration', '200', '1700'],\n",
       " ['20', 'Marketing', '201', '1800'],\n",
       " ['30', 'Purchasing', '114', '1700'],\n",
       " ['40', 'Human Resources', '203', '2400'],\n",
       " ['50', 'Shipping', '121', '1500'],\n",
       " ['60', 'IT', '103', '1400'],\n",
       " ['70', 'Public Relations', '204', '2700'],\n",
       " ['80', 'Sales', '145', '2500'],\n",
       " ['90', 'Executive', '100', '1700'],\n",
       " ['100', 'Finance', '108', '1700'],\n",
       " ['110', 'Accounting', '205', '1700']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter\n",
    "filtered_rdd = values_rdd.filter(lambda values: values[2] != ' - ')\n",
    "filtered_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'150,Shareholder Services, - ,1700'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_dept = data_rdd.reduce(lambda x, y: x if len(x.split(',')[1]) > len(y.split(',')[1]) else y)\n",
    "max_dept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_count =  11\n",
      "total_location_ids =  20800\n",
      "average_location_id =  1890.909090909091\n"
     ]
    }
   ],
   "source": [
    "# Aggregation\n",
    "total_count = filtered_rdd.count()\n",
    "total_location_ids = filtered_rdd.map(lambda values: int(values[3])).sum()\n",
    "average_location_id = total_location_ids / total_count\n",
    "\n",
    "print('total_count = ', total_count)\n",
    "print('total_location_ids = ', total_location_ids)\n",
    "print('average_location_id = ', average_location_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n",
      "|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE|    JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|\n",
      "+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n",
      "|        198|    Donald| OConnell|DOCONNEL|650.507.9833|21-JUN-07|  SH_CLERK|  2600|            - |       124|           50|\n",
      "|        199|   Douglas|    Grant|  DGRANT|650.507.9844|13-JAN-08|  SH_CLERK|  2600|            - |       124|           50|\n",
      "|        200|  Jennifer|   Whalen| JWHALEN|515.123.4444|17-SEP-03|   AD_ASST|  4400|            - |       101|           10|\n",
      "|        201|   Michael|Hartstein|MHARTSTE|515.123.5555|17-FEB-04|    MK_MAN| 13000|            - |       100|           20|\n",
      "|        202|       Pat|      Fay|    PFAY|603.123.6666|17-AUG-05|    MK_REP|  6000|            - |       201|           20|\n",
      "|        203|     Susan|   Mavris| SMAVRIS|515.123.7777|07-JUN-02|    HR_REP|  6500|            - |       101|           40|\n",
      "|        204|   Hermann|     Baer|   HBAER|515.123.8888|07-JUN-02|    PR_REP| 10000|            - |       101|           70|\n",
      "|        205|   Shelley|  Higgins|SHIGGINS|515.123.8080|07-JUN-02|    AC_MGR| 12008|            - |       101|          110|\n",
      "|        206|   William|    Gietz|  WGIETZ|515.123.8181|07-JUN-02|AC_ACCOUNT|  8300|            - |       205|          110|\n",
      "|        100|    Steven|     King|   SKING|515.123.4567|17-JUN-03|   AD_PRES| 24000|            - |        - |           90|\n",
      "|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568|21-SEP-05|     AD_VP| 17000|            - |       100|           90|\n",
      "|        102|       Lex|  De Haan| LDEHAAN|515.123.4569|13-JAN-01|     AD_VP| 17000|            - |       100|           90|\n",
      "|        103| Alexander|   Hunold| AHUNOLD|590.423.4567|03-JAN-06|   IT_PROG|  9000|            - |       102|           60|\n",
      "|        104|     Bruce|    Ernst|  BERNST|590.423.4568|21-MAY-07|   IT_PROG|  6000|            - |       103|           60|\n",
      "|        105|     David|   Austin| DAUSTIN|590.423.4569|25-JUN-05|   IT_PROG|  4800|            - |       103|           60|\n",
      "|        106|     Valli|Pataballa|VPATABAL|590.423.4560|05-FEB-06|   IT_PROG|  4800|            - |       103|           60|\n",
      "|        107|     Diana|  Lorentz|DLORENTZ|590.423.5567|07-FEB-07|   IT_PROG|  4200|            - |       103|           60|\n",
      "|        108|     Nancy|Greenberg|NGREENBE|515.124.4569|17-AUG-02|    FI_MGR| 12008|            - |       101|          100|\n",
      "|        109|    Daniel|   Faviet| DFAVIET|515.124.4169|16-AUG-02|FI_ACCOUNT|  9000|            - |       108|          100|\n",
      "|        110|      John|     Chen|   JCHEN|515.124.4269|28-SEP-05|FI_ACCOUNT|  8200|            - |       108|          100|\n",
      "+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"file:///root/ipynb/employees.csv\",header = True, inferSchema = True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+\n",
      "|FIRST_NAME|LAST_NAME|salary|\n",
      "+----------+---------+------+\n",
      "|    Donald| OConnell|  2600|\n",
      "|   Douglas|    Grant|  2600|\n",
      "|  Jennifer|   Whalen|  4400|\n",
      "|   Michael|Hartstein| 13000|\n",
      "|       Pat|      Fay|  6000|\n",
      "|     Susan|   Mavris|  6500|\n",
      "|   Hermann|     Baer| 10000|\n",
      "|   Shelley|  Higgins| 12008|\n",
      "|   William|    Gietz|  8300|\n",
      "|    Steven|     King| 24000|\n",
      "|     Neena|  Kochhar| 17000|\n",
      "|       Lex|  De Haan| 17000|\n",
      "| Alexander|   Hunold|  9000|\n",
      "|     Bruce|    Ernst|  6000|\n",
      "|     David|   Austin|  4800|\n",
      "|     Valli|Pataballa|  4800|\n",
      "|     Diana|  Lorentz|  4200|\n",
      "|     Nancy|Greenberg| 12008|\n",
      "|    Daniel|   Faviet|  9000|\n",
      "|      John|     Chen|  8200|\n",
      "+----------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter\n",
    "filter_df = df.filter(df.SALARY > 2000)\n",
    "filter_df.select('FIRST_NAME','LAST_NAME','salary' ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|    JOB_ID|count|\n",
      "+----------+-----+\n",
      "|FI_ACCOUNT|    5|\n",
      "|    MK_MAN|    1|\n",
      "|   IT_PROG|    5|\n",
      "|    FI_MGR|    1|\n",
      "|AC_ACCOUNT|    1|\n",
      "|    HR_REP|    1|\n",
      "|  PU_CLERK|    5|\n",
      "|    AC_MGR|    1|\n",
      "|    PR_REP|    1|\n",
      "|    ST_MAN|    5|\n",
      "|    MK_REP|    1|\n",
      "|    PU_MAN|    1|\n",
      "|  SH_CLERK|    2|\n",
      "|   AD_PRES|    1|\n",
      "|   AD_ASST|    1|\n",
      "|  ST_CLERK|   16|\n",
      "|     AD_VP|    2|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grouping\n",
    "from pyspark.sql.functions import col\n",
    "group_df = df.groupBy('JOB_ID').count()\n",
    "group_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+\n",
      "|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|\n",
      "+-----------+----------+---------+\n",
      "|        198|    Donald| OConnell|\n",
      "|        199|   Douglas|    Grant|\n",
      "|        200|  Jennifer|   Whalen|\n",
      "|        201|   Michael|Hartstein|\n",
      "|        202|       Pat|      Fay|\n",
      "|        203|     Susan|   Mavris|\n",
      "|        204|   Hermann|     Baer|\n",
      "|        205|   Shelley|  Higgins|\n",
      "|        206|   William|    Gietz|\n",
      "|        100|    Steven|     King|\n",
      "|        101|     Neena|  Kochhar|\n",
      "|        102|       Lex|  De Haan|\n",
      "|        103| Alexander|   Hunold|\n",
      "|        104|     Bruce|    Ernst|\n",
      "|        105|     David|   Austin|\n",
      "|        106|     Valli|Pataballa|\n",
      "|        107|     Diana|  Lorentz|\n",
      "|        108|     Nancy|Greenberg|\n",
      "|        109|    Daniel|   Faviet|\n",
      "|        110|      John|     Chen|\n",
      "+-----------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# JOIN\n",
    "\n",
    "df1 = df.select('FIRST_NAME','EMPLOYEE_ID')\n",
    "df2 = df.select('LAST_NAME','EMPLOYEE_ID')\n",
    "joined_df = df1.join(df2, on='EMPLOYEE_ID')\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|EMPLOYEE_ID|FIRST_NAME|\n",
      "+-----------+----------+\n",
      "|        203|     Susan|\n",
      "|        204|   Hermann|\n",
      "|        205|   Shelley|\n",
      "|        206|   William|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spark SQL Queries\n",
    "from pyspark.sql.functions import month\n",
    "df.createOrReplaceTempView('Employees')\n",
    "query = '''select EMPLOYEE_ID, FIRST_NAME\n",
    "           from Employees\n",
    "           where HIRE_DATE = '07-JUN-02' '''\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkStreamingKafkaApp\").getOrCreate()\n",
    "\n",
    "# Create a StreamingContext with a batch interval of 1 second\n",
    "ssc = StreamingContext(spark.sparkContext, 1)\n",
    "\n",
    "# Configure Kafka parameters\n",
    "brokers = \"localhost:9092\"  # Replace with your Kafka brokers\n",
    "topic = \"my_topic\"  # Replace with your Kafka topic\n",
    "\n",
    "# Create a DStream that represents the data stream from Kafka\n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {\"metadata.broker.list\": brokers})\n",
    "\n",
    "# Extract the values from Kafka messages\n",
    "lines = kafkaStream.map(lambda x: x[1])\n",
    "\n",
    "# Implement streaming transformations and actions on the DStream\n",
    "wordCounts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "                  .map(lambda word: (word, 1)) \\\n",
    "                  .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Print the word counts\n",
    "wordCounts.pprint()\n",
    "\n",
    "# Start the streaming context\n",
    "ssc.start()\n",
    "\n",
    "# Wait for the streaming to finish\n",
    "ssc.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quesiton - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sparkDriver = SparkSession.builder.master('local').appName(\"MySQL\").\\\n",
    "config('spark.jars.packages','ipynb/mysql-connector-j-8.0.33.tar.gz').\\\n",
    "getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mysql = sparkDriver.read.format('jdbc').\\\n",
    "option('url','jdbc:mysql://localhost:3306').\\\n",
    "option('driver','com.mysql.jdbc.Driver').\\\n",
    "option('user','root').\\\n",
    "option('password','Kumar@47').\\\n",
    "option('dbtable','class_3').\\\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mysql.createOrReplaceTempView(\"Employees\")\n",
    "result_df = spark.sql(\"select * from Employees\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
