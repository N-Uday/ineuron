{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce1b3a2e",
   "metadata": {},
   "source": [
    "## Question - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e247a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hadoop_config(file_path):\n",
    "    config = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith('#') or not line:\n",
    "                continue\n",
    "            key, value = line.split('=')\n",
    "            config[key.strip()] = value.strip()\n",
    "    return config\n",
    "\n",
    "# Example usage:\n",
    "config_file_path = '/path/to/hadoop/config.xml'\n",
    "hadoop_config = read_hadoop_config(config_file_path)\n",
    "print('Core components of Hadoop:')\n",
    "print('NameNode:', hadoop_config['dfs.namenode.http-address'])\n",
    "print('DataNodes:', hadoop_config['dfs.datanode.http-address'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3358c7",
   "metadata": {},
   "source": [
    "## Question - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fddf2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_hdfs_directory_size(directory):\n",
    "    try:\n",
    "        output = subprocess.check_output(['hadoop', 'fs', '-du', '-s', '-h', directory])\n",
    "        lines = output.decode().split('\\n')\n",
    "        total_size = lines[0].split()[0]\n",
    "        return total_size\n",
    "    except subprocess.CalledProcessError:\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "hdfs_directory = '/user/hadoop/data'\n",
    "total_size = get_hdfs_directory_size(hdfs_directory)\n",
    "if total_size:\n",
    "    print('Total size of', hdfs_directory, 'is', total_size)\n",
    "else:\n",
    "    print('Failed to retrieve the size of', hdfs_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12474d0",
   "metadata": {},
   "source": [
    "## Question - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14555f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from collections import Counter\n",
    "\n",
    "def mapper(line):\n",
    "    words = line.strip().split()\n",
    "    return Counter(words)\n",
    "\n",
    "def reducer(counters):\n",
    "    return sum(counters, Counter())\n",
    "\n",
    "def top_n_frequent_words(filename, n):\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    with Pool() as pool:\n",
    "        mapped_results = pool.map(mapper, lines)\n",
    "        reduced_result = reducer(mapped_results)\n",
    "\n",
    "    top_words = reduced_result.most_common(n)\n",
    "    return top_words\n",
    "\n",
    "# Example usage\n",
    "filename = 'large_text_file.txt'\n",
    "n = 10\n",
    "\n",
    "top_words = top_n_frequent_words(filename, n)\n",
    "for word, count in top_words:\n",
    "    print(f'{word}: {count}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0926e6df",
   "metadata": {},
   "source": [
    "## Question - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a832ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def check_hadoop_cluster_status():\n",
    "    namenode_url = 'http://<namenode-host>:<port>/jmx?qry=Hadoop:service=NameNode,name=NameNodeInfo'\n",
    "    datanode_url = 'http://<datanode-host>:<port>/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo'\n",
    "    \n",
    "    try:\n",
    "        namenode_response = requests.get(namenode_url)\n",
    "        namenode_data = namenode_response.json()\n",
    "        namenode_status = namenode_data['beans'][0]['State']\n",
    "        print('NameNode status:', namenode_status)\n",
    "\n",
    "        datanode_response = requests.get(datanode_url)\n",
    "        datanode_data = datanode_response.json()\n",
    "        datanode_count = len(datanode_data['beans'])\n",
    "        print('DataNode count:', datanode_count)\n",
    "        \n",
    "        return True\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print('Failed to connect to Hadoop cluster:', str(e))\n",
    "        return False\n",
    "\n",
    "# Example usage:\n",
    "check_hadoop_cluster_status()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01635360",
   "metadata": {},
   "source": [
    "## Question - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588d4f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def list_hdfs_path(path):\n",
    "    try:\n",
    "        output = subprocess.check_output(['hadoop', 'fs', '-ls', path])\n",
    "        lines = output.decode().split('\\n')\n",
    "        files = [line.split()[-1] for line in lines[1:] if line]\n",
    "        return files\n",
    "    except subprocess.CalledProcessError:\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "hdfs_path = '/user/hadoop/data'\n",
    "result = list_hdfs_path(hdfs_path)\n",
    "if result:\n",
    "    print('Files and directories in', hdfs_path, ':')\n",
    "    for file in result:\n",
    "        print(file)\n",
    "else:\n",
    "    print('Failed to list the path', hdfs_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafc04fb",
   "metadata": {},
   "source": [
    "## Question - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25a5e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def analyze_datanode_storage_utilization():\n",
    "    datanode_url = 'http://<datanode-host>:<port>/jmx?qry=Hadoop:service=DataNode,name=FSDatasetState'\n",
    "    \n",
    "    try:\n",
    "        datanode_response = requests.get(datanode_url)\n",
    "        datanode_data = datanode_response.json()\n",
    "        datanodes = datanode_data['beans'][0]['StorageInfo']\n",
    "        \n",
    "        datanode_usages = []\n",
    "        for datanode in datanodes:\n",
    "            capacity = datanode['capacity']\n",
    "            dfsUsed = datanode['dfsUsed']\n",
    "            datanode_usages.append((datanode['storageID'], dfsUsed / capacity))\n",
    "        \n",
    "        datanode_usages.sort(key=lambda x: x[1])\n",
    "        print('Datanode with highest storage capacity utilization:')\n",
    "        print('Storage ID:', datanode_usages[-1][0])\n",
    "        print('Utilization:', datanode_usages[-1][1])\n",
    "\n",
    "        print('Datanode with lowest storage capacity utilization:')\n",
    "        print('Storage ID:', datanode_usages[0][0])\n",
    "        print('Utilization:', datanode_usages[0][1])\n",
    "        \n",
    "        return True\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print('Failed to connect to Hadoop cluster:', str(e))\n",
    "        return False\n",
    "\n",
    "# Example usage:\n",
    "analyze_datanode_storage_utilization()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b84b5c",
   "metadata": {},
   "source": [
    "## Question - 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01da9a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(jar_path, input_path, output_path):\n",
    "    submit_url = 'http://<resourcemanager-host>:<port>/ws/v1/cluster/apps/new-application'\n",
    "    submit_response = requests.post(submit_url)\n",
    "    if submit_response.status_code == 200:\n",
    "        application_id = submit_response.json()['application-id']\n",
    "        print('Submitted Hadoop job. Application ID:', application_id)\n",
    "        \n",
    "        # Submit job\n",
    "        job_submit_url = f'http://<resourcemanager-host>:<port>/ws/v1/cluster/apps/{application_id}/appmaster/submit'\n",
    "        job_submit_data = {\n",
    "            'applicationId': application_id,\n",
    "            'jarPath': jar_path,\n",
    "            'inputPath': input_path,\n",
    "            'outputPath': output_path\n",
    "        }\n",
    "        requests.post(job_submit_url, json=job_submit_data)\n",
    "        \n",
    "        # Monitor progress\n",
    "        while True:\n",
    "            time.sleep(5)\n",
    "            job_info_url = f'http://<resourcemanager-host>:<port>/ws/v1/cluster/apps/{application_id}/appmaster/info'\n",
    "            job_info_response = requests.get(job_info_url)\n",
    "            job_info = job_info_response.json()\n",
    "            if job_info['state'] == 'FINISHED':\n",
    "                print('Job finished successfully.')\n",
    "                break\n",
    "            elif job_info['state'] == 'FAILED':\n",
    "                print('Job failed.')\n",
    "                break\n",
    "            else:\n",
    "                print('Job is still running. Progress:', job_info['progress'])\n",
    "    else:\n",
    "        print('Failed to submit the Hadoop job.')\n",
    "\n",
    "# Example usage:\n",
    "jar_path = '/path/to/hadoop-job.jar'\n",
    "input_path = '/input/path'\n",
    "output_path = '/output/path'\n",
    "submit_hadoop_job(jar_path, input_path, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979a46e0",
   "metadata": {},
   "source": [
    "## Question - 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec8964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job_with_resource_constraints(jar_path, input_path, output_path, memory, vcores):\n",
    "    submit_url = 'http://<resourcemanager-host>:<port>/ws/v1/cluster/apps/new-application'\n",
    "    submit_response = requests.post(submit_url)\n",
    "    if submit_response.status_code == 200:\n",
    "        application_id = submit_response.json()['application-id']\n",
    "        print('Submitted Hadoop job. Application ID:', application_id)\n",
    "        \n",
    "        # Submit job with resource requirements\n",
    "        job_submit_url = f'http://<resourcemanager-host>:<port>/ws/v1/cluster/apps/{application_id}/appmaster/submit'\n",
    "        job_submit_data = {\n",
    "            'applicationId': application_id,\n",
    "            'jarPath': jar_path,\n",
    "            'inputPath': input_path,\n",
    "            'outputPath': output_path,\n",
    "            'memory': memory,\n",
    "            'vcores': vcores\n",
    "        }\n",
    "        requests.post(job_submit_url, json=job_submit_data)\n",
    "        \n",
    "        # Monitor resource usage\n",
    "        while True:\n",
    "            time.sleep(5)\n",
    "            job_info_url = f'http://<resourcemanager-host>:<port>/ws/v1/cluster/apps/{application_id}/appmaster/info'\n",
    "            job_info_response = requests.get(job_info_url)\n",
    "            job_info = job_info_response.json()\n",
    "            if job_info['state'] == 'FINISHED':\n",
    "                print('Job finished successfully.')\n",
    "                break\n",
    "            elif job_info['state'] == 'FAILED':\n",
    "                print('Job failed.')\n",
    "                break\n",
    "            else:\n",
    "                resources_used = job_info['resourcesUsed']\n",
    "                print('Resources used:', resources_used['memory'], 'MB', resources_used['vcores'], 'vcores')\n",
    "    else:\n",
    "        print('Failed to submit the Hadoop job.')\n",
    "\n",
    "# Example usage:\n",
    "jar_path = '/path/to/hadoop-job.jar'\n",
    "input_path = '/input/path'\n",
    "output_path = '/output/path'\n",
    "memory = 4096  # 4GB\n",
    "vcores = 2\n",
    "submit_hadoop_job_with_resource_constraints(jar_path, input_path, output_path, memory, vcores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5eb4a3",
   "metadata": {},
   "source": [
    "## Question - 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3333a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "def run_mapreduce_job(input_path, output_path, split_size):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        subprocess.check_output(['hadoop', 'jar', 'path/to/hadoop-streaming.jar',\n",
    "                                 '-D', f'mapreduce.input.fileinputformat.split.maxsize={split_size}',\n",
    "                                 '-input', input_path,\n",
    "                                 '-output', output_path,\n",
    "                                 '-mapper', 'cat',\n",
    "                                 '-reducer', 'wc',\n",
    "                                 '-numReduceTasks', '1'])\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        return execution_time\n",
    "    except subprocess.CalledProcessError:\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "input_path = '/path/to/input_file.txt'\n",
    "output_path = '/output/path'\n",
    "split_sizes = [100000000, 500000000, 1000000000]  # Split sizes in bytes\n",
    "for split_size in split_sizes:\n",
    "    execution_time = run_mapreduce_job(input_path, output_path, split_size)\n",
    "    if execution_time:\n",
    "        print('Split size:', split_size, 'bytes')\n",
    "        print('Execution time:', execution_time, 'seconds')\n",
    "    else:\n",
    "        print('Failed to execute the MapReduce job with split size', split_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
